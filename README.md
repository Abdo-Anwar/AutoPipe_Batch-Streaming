# Mini ETL Data Pipeline (Postgres â†’ HDFS â†’ Spark â†’ Hive)

This is a **personal learning project** where I build an end-to-end ETL pipeline to practice Big Data tools and concepts.  
The pipeline demonstrates how raw data can be ingested, processed, and stored for analytics.

---

## ğŸš€ Current Features
- Extract data from **Postgres** using **Sqoop**.
- Store raw data in **HDFS**.
- Transform data using **PySpark**.
- Load results into **Hive tables** for analytical queries.
- Containerized environment using **Docker**.

---

## ğŸ› ï¸ Tech Stack
- **Postgres**
- **Sqoop**
- **HDFS**
- **PySpark**
- **Hive**
- **Docker**

---

## ğŸ“… Project Status
âœ… First working version completed.  
âš™ï¸ Currently working on improvements & new features.  
ğŸ—“ï¸ Next major update planned on **29 September 2025**.

---

## ğŸ”® Coming Soon
- **Apache Airflow** for orchestration.  
- **Data Quality checks** (Great Expectations).  
- **Automated CI/CD workflow** (GitHub Actions).  
- **Dashboard & visualization** for insights.  
