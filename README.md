# Mini ETL Data Pipeline (Postgres → HDFS → Spark → Hive)

This is a **personal learning project** where I build an end-to-end ETL pipeline to practice Big Data tools and concepts.  
The pipeline demonstrates how raw data can be ingested, processed, and stored for analytics.

---

## 🚀 Current Features
- Extract data from **Postgres** using **Sqoop**.
- Store raw data in **HDFS**.
- Transform data using **PySpark**.
- Load results into **Hive tables** for analytical queries.
- Containerized environment using **Docker**.

---

## 🛠️ Tech Stack
- **Postgres**
- **Sqoop**
- **HDFS**
- **PySpark**
- **Hive**
- **Docker**

---

## 📅 Project Status
✅ First working version completed.  
⚙️ Currently working on improvements & new features.  
🗓️ Next major update planned on **29 September 2025**.

---

## 🔮 Coming Soon
- **Apache Airflow** for orchestration.  
- **Data Quality checks** (Great Expectations).  
- **Automated CI/CD workflow** (GitHub Actions).  
- **Dashboard & visualization** for insights.  
